{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb74a2d9",
   "metadata": {
    "id": "fb74a2d9"
   },
   "source": [
    "# Tweets analyzer.\n",
    "\n",
    "#### This Notebook analyzes tweets to from individuals to a company. The please note:\n",
    "##### 1. The main goal of this notebook is to analyze questions and reviews left by users for company's therefore to get more relevant data I have excluded tweets with attachments and only included tweets which are not replies to tweets or retweets.\n",
    "\n",
    "#### 2. The data has been scrapped from twitter directly using octoparse 8. This data is from the periods between 01/01/2021 to 31/12/2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2e3f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9d2e3f4",
    "outputId": "4e0c5348-6558-4cbb-e98b-04a9186e1f33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\user\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (60.5.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (60.5.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 12:39:14.185939: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-02 12:39:14.186009: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: spacy-langdetect in c:\\users\\user\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: pytest in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy-langdetect) (6.2.3)\n",
      "Requirement already satisfied: langdetect==1.0.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy-langdetect) (1.0.7)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from langdetect==1.0.7->spacy-langdetect) (1.15.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (20.3.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\user\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (20.9)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (1.10.0)\n",
      "Requirement already satisfied: toml in c:\\users\\user\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (0.10.2)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from pytest->spacy-langdetect) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->pytest->spacy-langdetect) (2.4.7)\n",
      "Requirement already satisfied: contractions in c:\\users\\user\\anaconda3\\lib\\site-packages (0.1.66)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: anyascii in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\user\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install spacy-langdetect\n",
    "!pip install contractions\n",
    "!pip install gensim\n",
    "!pip install pyLDAvis\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import datetime\n",
    "import spacy\n",
    "import nltk\n",
    "import contractions\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lfegOKyh5Km3",
   "metadata": {
    "id": "lfegOKyh5Km3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7ab50",
   "metadata": {
    "id": "c9b7ab50"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/kplc_twitter_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2a020",
   "metadata": {
    "id": "efe2a020"
   },
   "outputs": [],
   "source": [
    "def convertCountToInt(count):\n",
    "    if(isinstance(count,str) and count[-1].lower() in ['k','m']):\n",
    "        multiplier = 1000  if count[-1].lower() == 'k' else 1000000\n",
    "        count = float(count[:len(count)-1]) * multiplier\n",
    "    return int(count)\n",
    "\n",
    "def cleanTweet1(tweet):\n",
    "    pattern = r'\\n'\n",
    "    return re.sub(pattern,' ',tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c8e72",
   "metadata": {
    "id": "db6c8e72"
   },
   "outputs": [],
   "source": [
    "#converting comments, reshare and likes to int\n",
    "df.fillna(0,inplace=True)\n",
    "\n",
    "intColumns = ['comments','reshare','likes']\n",
    "\n",
    "for column in intColumns:\n",
    "    df[column] = df[column].apply(convertCountToInt);\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a7e30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b88a7e30",
    "outputId": "8a574dfb-4f2d-43e4-97bd-b04d92ead605"
   },
   "outputs": [],
   "source": [
    "#we'll remove next line tags to cleanup the data\n",
    "df['tweet_cleaned'] = df['tweet'].apply(cleanTweet1)\n",
    "\n",
    "#checking whether there are any null columns\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97072f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "7f97072f",
    "outputId": "2c652fad-c9ab-4fe4-eeff-b8648060ac90"
   },
   "outputs": [],
   "source": [
    "#we'll remove the name column since we don't need it for the analysis\n",
    "df.drop(['Name'],axis=1,inplace = True)\n",
    "\n",
    "df.sample(10, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ea005",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "501ea005",
    "outputId": "1a924417-8ee0-4d38-d987-ed2ea2685010"
   },
   "outputs": [],
   "source": [
    "# we need to format the date in a more analsis friendly way\n",
    "df['Month'] = pd.DatetimeIndex(df['Time']).month\n",
    "df['Day'] = pd.DatetimeIndex(df['Time']).day\n",
    "df['Time'] = pd.DatetimeIndex(df['Time']).time\n",
    "\n",
    "df.sample(10, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d786b",
   "metadata": {
    "id": "d85d786b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "LanguageClassifier = spacy.load('en')\n",
    "LanguageClassifier.add_pipe(LanguageDetector(), name='language_detector', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499c11f",
   "metadata": {
    "id": "b499c11f"
   },
   "outputs": [],
   "source": [
    "#since we have tweets in both swahili and english, we will first categorize them into the respective language.\n",
    "df['language'] = df['tweet'].apply(lambda tweet: 'Swahili' if LanguageClassifier(tweet)._.language['language']== 'sw' else 'English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb0fc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "id": "8ebb0fc0",
    "outputId": "f070b92a-19f2-4215-9189-49d6f83763f9"
   },
   "outputs": [],
   "source": [
    "df.sample(10, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fecd7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "45fecd7c",
    "outputId": "1db5f5c2-b2f0-4795-d0ae-4dab9b92ff7b"
   },
   "outputs": [],
   "source": [
    "#Lets see the count of kiswahili vs English reviews\n",
    "df['language'].value_counts()\n",
    "\n",
    "fig = px.bar(df, \n",
    "             y=df['language'].value_counts().values, \n",
    "             x=df['language'].value_counts().index,\n",
    "             title = \"Counts of tweets by language\",\n",
    "            labels={\"x\":\"Language\",\"y\":\"Number of tweets\"})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91141d13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "91141d13",
    "outputId": "faf8508a-f595-4447-c9e6-dce9243de744"
   },
   "outputs": [],
   "source": [
    "#Now let's get a feel of the average length of a tweet\n",
    "\n",
    "df['tweetLength'] = df['tweet'].apply(lambda tweet:len(tweet))\n",
    "fig = px.box(df,y='tweetLength',title=\"Average tweet length\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96ba40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "2c96ba40",
    "outputId": "9f690ba8-7f6b-4157-ab3b-6faa3f68ebfe"
   },
   "outputs": [],
   "source": [
    "#lets get to see the distribution of the tweets over different months\n",
    "\n",
    "fig = px.bar(df,\n",
    "             y=df['Month'].value_counts().values, \n",
    "             x=df['Month'].value_counts().index,\n",
    "             title = \"Counts of tweets by Month\",\n",
    "            labels={\"x\":\"Month\",\"y\":\"Number of tweets\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09742f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "cd09742f",
    "outputId": "f7a3a648-d545-4b22-cc8c-9f93491d4a86"
   },
   "outputs": [],
   "source": [
    "#Getting distribution on accordance to hour of the day\n",
    "\n",
    "df['hour'] = df['Time'].apply(lambda time: time.hour)\n",
    "\n",
    "fig = px.bar(df,\n",
    "             y=df['hour'].value_counts().values, \n",
    "             x=df['hour'].value_counts().index,\n",
    "             title = \"Counts of tweets by Month\",\n",
    "            labels={\"x\":\"Hour\",\"y\":\"Number of tweets\"},\n",
    "            )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ffb2c6",
   "metadata": {
    "id": "e4ffb2c6"
   },
   "source": [
    "#### From the initial EDA we have the following info:\n",
    "#### 1. The tweets are multilingual with the larger percentage being on English and the lesser is Kiswahili.\n",
    "#### 2. The average length of a tweet is 148 characters which tells us that people don't write long tweets for this company\n",
    "#### 3. The first quarter of the year saw relatively more tweets than the other quarters.\n",
    "#### 4. Most people tweet in the morning hours (6am - 9am) and in the afternoon (2pm - 6pm)\n",
    "\n",
    "#### We will proceed to analyze the English reviews since they are more as compared to the English once and we also have more resources for English compared to swahili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969b5ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "1969b5ed",
    "outputId": "d2d610e4-c5eb-4538-b674-9408c5fe1be1"
   },
   "outputs": [],
   "source": [
    "#We'll first of all get our subset of the English reviews\n",
    "\n",
    "df_english = df[df['language'] == 'English'].copy()\n",
    "df_english.drop(['comments','reshare','likes','Time','language'], axis = 1, inplace= True)\n",
    "df_english.sample(10, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c4cec",
   "metadata": {
    "id": "860c4cec"
   },
   "outputs": [],
   "source": [
    "#Lets no clean the data to so a more indepth analysis\n",
    "\n",
    "#removing contractions, special charaters\n",
    "def cleanTweet2(tweet):\n",
    "  tweet = tweet.lower()\n",
    "  tweet = [contractions.fix(word) for word in tweet.split()]\n",
    "  tweet = ' '.join(map(str, tweet)) \n",
    "  tweet = re.sub('[^\\w\\d\\s]+','',tweet)\n",
    "  return tweet\n",
    "\n",
    "\n",
    "df_english['tweet_cleaned2'] = df_english['tweet_cleaned'].apply(cleanTweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ccb6bf",
   "metadata": {
    "id": "b0ccb6bf"
   },
   "outputs": [],
   "source": [
    "#iteratively going through the most common words to remove those that don't provide us with meaning.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_words(dataframe,ngram,n=None,):\n",
    "  vector = CountVectorizer(ngram_range=(ngram, ngram)).fit(dataframe)\n",
    "  bag_of_words = vector.transform(dataframe)\n",
    "  sum_words = bag_of_words.sum(axis=0) \n",
    "  words_freq = [(word, sum_words[0, idx]) for word, idx in vector.vocabulary_.items()]\n",
    "  words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "  return pd.DataFrame(words_freq[:n],columns=['words','count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zVqGe0CKHJL2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "zVqGe0CKHJL2",
    "outputId": "a24ae2fc-8a61-4326-dbba-83be4382618e"
   },
   "outputs": [],
   "source": [
    "#checking bigrams to see which stop words bring meaning to statements\n",
    "\n",
    "fig =px.bar(get_top_n_words(df_english['tweet_cleaned2'],2,50),x='words',y='count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jw6vFEVKiq8u",
   "metadata": {
    "id": "jw6vFEVKiq8u"
   },
   "outputs": [],
   "source": [
    "#well remove the twitter mentions for kenya power since it is skewing the results and remove no from stop word since it has meaning in our statements\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words('english'))\n",
    "stop.update(['kenyapower_care','kenyapower','kenyapoweralert','hey','hi','hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "siyGSoHbjdps",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "siyGSoHbjdps",
    "outputId": "14eec2cf-5de0-4a73-ebb5-a67fc2727dde"
   },
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df_english['tweet_cleaned2'] = df_english['tweet_cleaned2'].apply(word_tokenize)\n",
    "df_english['tweet_cleaned2'] = df_english['tweet_cleaned2'].apply(lambda x: [word for word in x if word not in stop])\n",
    "df_english['tweet_cleaned2'] = [' '.join(map(str, l)) for l in df_english['tweet_cleaned2']]\n",
    "fig =px.bar(get_top_n_words(df_english['tweet_cleaned2'],2,50),x='words',y='count')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yX3dss19jZYp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "yX3dss19jZYp",
    "outputId": "9c2ff4e0-8375-4e68-9b39-04537706307a"
   },
   "outputs": [],
   "source": [
    "fig =px.bar(get_top_n_words(df_english['tweet_cleaned2'],3,50),x='words',y='count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jfqJ9laNlYMJ",
   "metadata": {
    "id": "jfqJ9laNlYMJ"
   },
   "outputs": [],
   "source": [
    "#Lamentization\n",
    "df_english['tweet_cleaned2'] = df_english['tweet_cleaned2'].apply(word_tokenize)\n",
    "\n",
    "df_english['pos_tags'] = df_english['tweet_cleaned2'].apply(nltk.tag.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2e76e",
   "metadata": {
    "id": "25d2e76e"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "df_english['wordnet_pos'] = df_english['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D4ZOi7tfnCZZ",
   "metadata": {
    "id": "D4ZOi7tfnCZZ"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "df_english['lemmatized'] = df_english['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4GbQj2KnpNh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "p4GbQj2KnpNh",
    "outputId": "089e945b-b7fa-4d7e-aaf5-211fdd2ffaa9"
   },
   "outputs": [],
   "source": [
    "df_english['lemmatized_joined'] = [' '.join(map(str, l)) for l in df_english['lemmatized']]\n",
    "\n",
    "fig =px.bar(get_top_n_words(df_english['lemmatized_joined'],2,50),x='words',y='count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GmQ27vr85R90",
   "metadata": {
    "id": "GmQ27vr85R90"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4oc69XJhoqbS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "4oc69XJhoqbS",
    "outputId": "28a1c9fb-077c-406e-fbd6-a8ce9e81fca4"
   },
   "outputs": [],
   "source": [
    "#we need to replace similar words which have been written differently\n",
    "\n",
    "replacementKey = {\n",
    "    'meter no':' meter number ',\n",
    "    'metre no':' meter number ',\n",
    "    'acc no':' account number ',\n",
    "    'metre':' meter ',\n",
    "    'acc': ' account ',\n",
    "    'ac': ' account ',\n",
    "    'account no':' account number ',\n",
    "    'mtr':' meter '}\n",
    "\n",
    "def replaceWord(tweet):\n",
    "  for word,word2 in replacementKey.items():\n",
    "    tweet = re.sub('\\s' + word + '\\s',word2,tweet)\n",
    "  return tweet\n",
    "\n",
    "df_english['lemmatized_joined'] = df_english['lemmatized_joined'].apply(replaceWord)\n",
    "\n",
    "fig =px.bar(get_top_n_words(df_english['lemmatized_joined'],2,50),x='words',y='count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9_epT8P5TeA",
   "metadata": {
    "id": "f9_epT8P5TeA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_8M13OdW5NFj",
   "metadata": {
    "id": "_8M13OdW5NFj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EPJUffkQrDkA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "EPJUffkQrDkA",
    "outputId": "a9239348-f75a-4ad6-8b89-cd5c83b1a56e"
   },
   "outputs": [],
   "source": [
    "#let's see a word cloud of the most common terms\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "text = df_english['lemmatized_joined'].tolist() \n",
    "\n",
    "\n",
    "text = ' '.join(text)\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(collocations=True,width=800, height=400).generate(text)\n",
    "\n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud, interpolation='bilInear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wr1SjrGdQX_v",
   "metadata": {
    "id": "Wr1SjrGdQX_v"
   },
   "source": [
    "### From the word cloud we realize that some of the words are repeated many times and is worth investigation to see whether it is a sign of dirty data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HcBQo1KuQuo6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "HcBQo1KuQuo6",
    "outputId": "aa803404-48a0-4d80-9865-d9b2abb434e6"
   },
   "outputs": [],
   "source": [
    "#checking the most common words\n",
    "\n",
    "fig =px.bar(get_top_n_words(df_english['lemmatized_joined'],1,50),x='words',y='count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TnLF9QTmQyBX",
   "metadata": {
    "id": "TnLF9QTmQyBX"
   },
   "outputs": [],
   "source": [
    "mostCommon = get_top_n_words(df_english['lemmatized_joined'],1,150)\n",
    "indexes = []\n",
    "for word in mostCommon['words']:\n",
    "  sentences = df_english[df_english['lemmatized_joined'].str.contains(word)]\n",
    "  if(sentences['handle'].nunique()/len(sentences) * 100 < 10):\n",
    "    users = sentences['handle'].unique();\n",
    "    for user in users:\n",
    "      indexes += (df_english[df_english['lemmatized_joined'].str.contains(word) & df_english['handle'].str.contains(user)][1:].index.tolist())\n",
    "indexes = list(dict.fromkeys(indexes))\n",
    "df_english.drop(indexes,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XwTDrui5Yp_d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "XwTDrui5Yp_d",
    "outputId": "78e13a48-0f3d-4588-9a8c-2461e1c58b29"
   },
   "outputs": [],
   "source": [
    "fig =px.bar(get_top_n_words(df_english['lemmatized_joined'],1,50),x='words',y='count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GbbldTss2GSz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "GbbldTss2GSz",
    "outputId": "8c0c2974-bb03-43e2-d86a-b2621d92149b"
   },
   "outputs": [],
   "source": [
    "text = df_english['lemmatized_joined'].tolist() \n",
    "\n",
    "\n",
    "text = ' '.join(text)\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(collocations=True,width=800, height=400).generate(text)\n",
    "\n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud, interpolation='bilInear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lxxcy__AqggA",
   "metadata": {
    "id": "lxxcy__AqggA"
   },
   "source": [
    "### Lets do some topic modeling on this data to check whether we could be able to categorize the tweets.\n",
    "\n",
    "### We will start by trying LSA technique to see if we get sensible topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iKV0bPlYtoyd",
   "metadata": {
    "id": "iKV0bPlYtoyd"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1,3),\n",
    "    stop_words='english',\n",
    "    max_features=100\n",
    "    )\n",
    "\n",
    "vectors = vectorizer.fit_transform(df_english['lemmatized_joined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vuVvQ6BCuRq5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuVvQ6BCuRq5",
    "outputId": "ed9864d2-1d74-45f7-846d-601ba3d2b60a"
   },
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=2,n_iter=1000)\n",
    "\n",
    "lsa.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JxyGFkRjuUK2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxyGFkRjuUK2",
    "outputId": "15056322-27cd-47dd-82ed-f004fb1e6822"
   },
   "outputs": [],
   "source": [
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, vect in enumerate(lsa.components_):\n",
    "  voc_vect = zip(vocabulary,vect)\n",
    "  sorted_voc = sorted(voc_vect,key = lambda x:x[1], reverse=True)[:15]\n",
    "  print(\"Topic %d\" % i)\n",
    "  for term in sorted_voc:\n",
    "    print(term[0])\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1uPVvdK_7vGM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "1uPVvdK_7vGM",
    "outputId": "87f6357b-a416-4444-d763-6eee7ebd8480"
   },
   "outputs": [],
   "source": [
    "cluster_model = KMeans(n_clusters=2, max_iter=100,n_init=4)\n",
    "\n",
    "cluster_model.fit(vectors)\n",
    "\n",
    "clusters = cluster_model.predict(vectors)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "scatter_plots = pca.fit_transform(vectors.toarray())\n",
    "\n",
    "colors = [\"topic 1\",\"topic 2\",\"topic 3\"]\n",
    "\n",
    "x_axis = [o[0] for o in scatter_plots]\n",
    "y_axis = [o[1] for o in scatter_plots]\n",
    "\n",
    "fig = px.scatter(x= x_axis, y = y_axis, color = [colors[d] for d in clusters])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3hUGUS1HCf__",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hUGUS1HCf__",
    "outputId": "ca2295fb-7d7d-4a1a-b03b-821dfa6f6b78"
   },
   "outputs": [],
   "source": [
    "clustersProb = cluster_model.cluster_centers_.argsort()[:,::-1]\n",
    "\n",
    "for index_cluster in range(2):\n",
    "  print(\"Topic %d\" % index_cluster)\n",
    "  for ind in clustersProb[index_cluster,:15]:\n",
    "    print(vocabulary[ind])\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hJYluMaf5FPF",
   "metadata": {
    "id": "hJYluMaf5FPF"
   },
   "source": [
    "### From LSA we can clearly tell that we have 2 main categories for tweet queries:\n",
    "\n",
    "1. Power outage.\n",
    "\n",
    "2. Token Enquries.\n",
    "\n",
    "### Lets now do an analysis using LDA to see if we are going to get more insightful data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sy9UdZHq5h6Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy9UdZHq5h6Y",
    "outputId": "86331bf8-79f2-4aa8-c77f-bfd8ab82ad34"
   },
   "outputs": [],
   "source": [
    "\n",
    "textTokenized = df_english['lemmatized_joined'].apply(word_tokenize)\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(textTokenized, min_count=5, threshold=100) \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "textP = make_bigrams(textTokenized)\n",
    "\n",
    "id2word = corpora.Dictionary(textP)\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in textP]\n",
    "\n",
    "tfidf = TfidfModel(corpus,id2word)\n",
    "\n",
    "max_df = 0.03\n",
    "words = []\n",
    "words_missing = []\n",
    "for i in range (0, len(corpus)):\n",
    "  bow = corpus[i]\n",
    "  low_value_words = []\n",
    "  tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "  bow_ids = [id for id,value in bow]\n",
    "  low_value_words = [id for id, value in tfidf[bow] if value < max_df]\n",
    "  drops = low_value_words + words_missing\n",
    "  for item in drops:\n",
    "    words.append(id2word[item])\n",
    "  words_missing = [id for id in bow_ids if id not in tfidf_ids]\n",
    "  new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing]\n",
    "  corpus[i] = new_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vORVeT4cjcUJ",
   "metadata": {
    "id": "vORVeT4cjcUJ"
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=2, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aUpZZ_UiU02u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "id": "aUpZZ_UiU02u",
    "outputId": "f97d14ca-12ff-47df-af5a-7c5efcdf8d1a"
   },
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus,id2word,mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dI_ARWUAXqOx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dI_ARWUAXqOx",
    "outputId": "cb556fe5-debd-46db-9dcf-054fece2a641"
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=textTokenized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of tweetNotebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
